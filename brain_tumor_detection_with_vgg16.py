# -*- coding: utf-8 -*-
"""Brain Tumor Detection with VGG16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E4-RpvYqPa2TM4sU9webDYdFhyiaDPGU
"""

!unzip '/content/drive/MyDrive/Brain.zip' -d Brain

import os
import tensorflow as tf
from tensorflow import keras 
import numpy as np
import matplotlib.pyplot as plt
import cv2 as cv
import seaborn as sns
from random import choices
from keras.callbacks import EarlyStopping,ModelCheckpoint
from keras.applications.vgg16 import VGG16
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Dense,Conv2D,MaxPooling2D,BatchNormalization,Flatten,Dropout

input_path='/content/Brain/brain_tumor_dataset'
for file in os.listdir(input_path):
    print(file)

#Let's visualize the total number of labels of each type in data
type1=len(os.listdir(input_path+'/no'))
type2=len(os.listdir(input_path+'/yes'))

count=[type1,type2]
label=['Tumor','Normal']

sns.barplot(label,count)

def show_image(folder):
    path=os.path.join(input_path,folder)
    
    images=choices(os.listdir(path),k=4)
    images=[os.path.join(path,file) for file in images]
    
    return images

#Let's see how the MRI scan of each type looks. We would be using open cv library to read and show the image.
img1=show_image('no')
img2=show_image('yes')
label1=['no']*4
label2=['yes']*4

images=img1+img2
labels=label1+label2

plt.figure(figsize=(16,15))

for i,path_name in enumerate(images):
    plt.subplot(4,2,i+1)
    image=cv.imread(path_name)
    plt.imshow(image)
    plt.title("Tumor Present:"+labels[i])
    plt.axis('off')

"""**Model Building**"""

#Image Augmentation
#We will use Image Augmentation to train the model on different types of combination formed by rotation ,flipping of image so as to increase our model accuracy
datagen = ImageDataGenerator(rescale=1/255,
                             rotation_range=20,
                             horizontal_flip=True,
                             height_shift_range=0.1,
                             width_shift_range=0.1,
                             shear_range=0.1,
                             brightness_range=[0.3, 1.5],
                             validation_split=0.2
                            )

train_gen= datagen.flow_from_directory(input_path,
                                       target_size=(224,224),
                                       class_mode='binary',
                                       subset='training'
                                      )
val_gen = datagen.flow_from_directory( input_path,
                                       target_size=(224,224),
                                       class_mode='binary',
                                       subset='validation'
                                      )

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Dense, Dropout,Flatten

# img_rows,img_cols=28,28
# if k.backend.image_data_format() == 'channels_first':
#     #X_train = Tk.reshape(1000, 1, img_rows, img_cols)
#     #X_train = X_train.reshape()
#     X_train = train_gen.reshape(train_gen.shape[0], 1, img_rows, img_cols)
#     val_gen = val_gen.reshape(val_gen.shape[0], 1, img_rows, img_cols)
#     input_shape = (1, img_rows, img_cols)

# ##model building
# model = Sequential()
# #convolutional layer with rectified linear unit activation
# model.add(Conv2D(32, kernel_size=(3, 3),
#                  activation='relu',
#                  input_shape=input_shape))
# #32 convolution filters used each of size 3x3
# #again
# model.add(Conv2D(64, (3, 3), activation='relu'))
# #64 convolution filters used each of size 3x3
# #choose the best features via pooling
# model.add(MaxPooling2D(pool_size=(2, 2)))
# #randomly turn neurons on and off to improve convergence
# model.add(Dropout(0.25))
# #flatten since too many dimensions, we only want a classification output
# model.add(Flatten())
# #fully connected to get all relevant data
# model.add(Dense(128, activation='relu'))
# #one more dropout for convergence' sake :)
# model.add(Dropout(0.5))
# #output a softmax to squash the matrix into output probabilities
# model.add(Dense(num_category, activation='softmax'))
# #Adaptive learning rate (adaDelta) is a popular form of gradient descent rivaled only by adam and adagrad
# #categorical ce since we have multiple classes (10)
# model.compile(loss=keras.losses.categorical_crossentropy,
#               optimizer=keras.optimizers.Adadelta(),
#               metrics=['accuracy'])

vgg_model=VGG16(weights='imagenet',input_shape=(224,224,3),include_top=False)
model=keras.Sequential()
for layer in vgg_model.layers:
    model.add(layer)
for layer in model.layers:
    layer.trainable=False

#We will add dropout and other dense layers to avoid overfitting and make the model more robust.
model.add(Flatten())
model.add(Dropout(0.4))
model.add(Dense(256,activation='relu'))
model.add(Dense(1,activation='sigmoid'))
model.summary()

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy']
             )

stop = EarlyStopping(
    monitor='val_accuracy', 
    mode='max',
    patience=30
)

checkpoint= ModelCheckpoint(
    filepath='./',
    save_weights_only=True,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)

history=model.fit(train_gen,validation_data=val_gen,epochs=30,callbacks=[stop,checkpoint])

#Model Performance
plt.plot(history.history['loss'],label='train loss')
plt.plot(history.history['val_loss'],label='validation loss')
plt.legend()

plt.plot(history.history['accuracy'],label='train accuracy')
plt.plot(history.history['val_accuracy'],label='validation accuracy')
plt.legend()

print("training_accuracy", history.history['accuracy'][-1])
print("validation_accuracy", history.history['val_accuracy'][-1])